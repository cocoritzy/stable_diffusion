{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8e0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5099fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##Â Load Pretrained Stable Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b859a8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cb996595ca4f5d8cc97250b96b4682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the base SDXL model\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7fdc2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049d505d9e574cbf83520d6874ff6555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = (\"A man\")\n",
    "\n",
    "image_sb=pipe(prompt).images[0]\n",
    "image_sb.save(\"a_man_sd.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaca1bd",
   "metadata": {},
   "source": [
    "## Configure LoRA for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4a92a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    }
   ],
   "source": [
    "# Load the IKEA instructions LoRA\n",
    "pipe.load_lora_weights(\n",
    "    \"ostris/ikea-instructions-lora-sdxl\",\n",
    "    adapter_name=\"ikea\"\n",
    ")\n",
    "\n",
    "# Activate the adapter\n",
    "pipe.set_adapters(\"ikea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee88cbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd315a42527a437bb2984a3eabe7ee9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = (\"A man\")\n",
    "\n",
    "image_ikea=pipe(prompt).images[0]\n",
    "image_ikea.save(\"a_man_ikea.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcaa72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    }
   ],
   "source": [
    "# Load the IKEA instructions LoRA\n",
    "pipe.load_lora_weights(\"CiroN2022/toy-face\", adapter_name=\"toy-face\")\n",
    "\n",
    "# Activate the adapter\n",
    "pipe.set_adapters(\"toy-face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11e454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d993b3831dfa49429d8a395e83207404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = (\"A man\")\n",
    "\n",
    "image_toy_face=pipe(prompt).images[0]\n",
    "image_toy_face.save(\"a_man_toy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c493206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeef8395395c46e0ab38212f44c9be44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe.load_lora_weights(\"ostris/super-cereal-sdxl-lora\", adapter_name=\"cereal\")\n",
    "# # Load the IKEA instructions LoRA\n",
    "# pipe.load_lora_weights(\"CiroN2022/toy-face\", adapter_name=\"toy-face\")\n",
    "\n",
    "# # Activate the adapter\n",
    "pipe.set_adapters(\"cereal\")\n",
    "prompt = (\"A man\")\n",
    "\n",
    "image_toy_face=pipe(prompt).images[0]\n",
    "image_toy_face.save(\"a_man_cereal.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
